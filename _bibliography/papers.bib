---
---

@inproceedings{ge2025framemindframeinterleavedvideoreasoning,
      title={FrameMind: Frame-Interleaved Video Reasoning via Reinforcement Learning}, 
      author={Haonan Ge and Yiwei Wang and Kai-Wei Chang and Hang Wu and Yujun Cai},
      booktitle={EMNLP},
      year={2025},
      url={https://arxiv.org/abs/2509.24008}, 
      selected={true},
      abbr         = {ICLR},
      note={Under review.}
      arxiv        = {2509.24008}, 
      abstract={Current video understanding models rely on fixed frame sampling strategies, processing predetermined visual inputs regardless of the specific reasoning requirements of each question. This static approach limits their ability to adaptively gather visual evidence, leading to suboptimal performance on tasks that require either broad temporal coverage or fine-grained spatial detail. In this paper, we introduce FrameMind, an end-to-end framework trained with reinforcement learning that enables models to dynamically request visual information during reasoning through Frame-Interleaved Chain-of-Thought (FiCOT). Unlike traditional approaches, FrameMind operates in multiple turns where the model alternates between textual reasoning and active visual perception, using tools to extract targeted frames or video clips based on identified knowledge gaps. To train effective dynamic sampling policies, we propose Dynamic Resolution Frame Sampling (DRFS), which exposes models to diverse temporal-spatial trade-offs during learning, and DRFS-GRPO, a group-relative policy optimization algorithm that learns from outcome-based rewards without requiring frame-level annotations. Extensive experiments on challenging benchmarks like MLVU and VideoMME demonstrate that our method significantly outperforms existing models, advancing the state of the art in flexible and efficient video understanding.},
      pdf          = {https://www.arxiv.org/pdf/2509.24008},
      preview      = {assets/img/Framemind.png}
}

@inproceedings{ge2025mrfdmultiregionfusiondecoding,
      title={MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs}, 
      author={Haonan Ge and Yiwei Wang and Ming-Hsuan Yang and Yujun Cai},
      booktitle={Under review}
      year={2025},
      url={https://arxiv.org/abs/2508.10264}, 
      selected={true},
      abbr         = {EMNLP},
      arxiv        = {2508.10264}, 
      abstract={Large Vision-Language Models (LVLMs) have shown strong performance across multimodal tasks. However, they often produce hallucinations -- text that is inconsistent with visual input, due to the limited ability to verify information in different regions of the image. To address this, we propose Multi-Region Fusion Decoding (MRFD), a training-free decoding method that improves factual grounding by modeling inter-region consistency. MRFD identifies salient regions using cross-attention, generates initial responses for each, and computes reliability weights based on Jensen-Shannon Divergence (JSD) among the responses. These weights guide a consistency-aware fusion of per-region predictions, using region-aware prompts inspired by Chain-of-Thought reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD significantly reduces hallucinations and improves response factuality without requiring model updates.},
      pdf          = {https://www.arxiv.org/pdf/2508.10264},
      preview      = {assets/img/MRFD.png}
}
